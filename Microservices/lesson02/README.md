# Задача 1: Обеспечить разработку

### Архитектура
1 - Для хранения кода и систему контроля версий используем GitLab по одному репозиторию на каждый микросервис с единой политикой.

2 - Настраиваем CI/CD через механизмы GitLab с собственными раннерами на серверах и развёртывания по изменениям в репозитории манифестов.

3 - Используем Yandex Container Registry как реестр контейнеров для хранения собственных докер-образы для сборки проектов.

4 - Используем Yandex Lockbox для хранения облачное хранилище секретов для временных доступов без постоянных ключей.

### Концепция:
1 - Разработчик отправляют код в репозиторий сервиса.

2 - GitLab генерирует событие, CI стартует автоматически.

3 - Ручные запуски доступны из интерфейса CI с параметрами (окружение, флаги, версии).

4 - Сборка контейнера публикуется в реестр Yandex Container Registry .

5 - CD не трогает само приложение напрямую. Обновляется репозиторий развёртывания.

6 - Секреты не хранятся в CI. Пайплайн получает токены и обращается к Yandex Lockbox, а само приложение — через оператор внешних секретов.

### Чек-лист
- облачная система; - да (GitLab, Yandex Container Registry, Yandex Lockbox)
- система контроля версий Git; - да (GitLab)
- репозиторий на каждый сервис; - да (у каждого микросервиса свой репозиторий)
- запуск сборки по событию из системы контроля версий; - да (автотриггеры на push)
- запуск сборки по кнопке с указанием параметров; - да (ручной запуск с inputs/variables)
- возможность привязать настройки к каждой сборке; да (переменные/секреты на окружение).
- возможность создания шаблонов для различных конфигураций сборок; - да
- возможность безопасного хранения секретных данных (пароли, ключи доступа); - да (Yandex Lockbox))
- несколько конфигураций для сборки из одного репозитория; - да (несколько пайплайнов, профили, фильтры путей/веток).
- кастомные шаги при сборке; -да (любой конвейер из стадий)
- собственные докер-образы для сборки проектов; - да (Yandex Container Registry)
- возможность развернуть агентов сборки на собственных серверах; - да (self-hosted runners)
- возможность параллельного запуска нескольких сборок; - да (матрицы, независимые джобы)
- возможность параллельного запуска тестов. - да (шардирование/распараллеливание)





# Задача 2: Логи

### Концепция:
1 - Используем Grafana: UI + хранилище логов.

2 - Агент на каждом хосте/узле: Promtail для сбора метрик.

3 - Приложения пишут логи в stdout/stderr.

Я считаю, что Grafana + Promtail (stdout→Loki) простой и хороший старт. 
С такой реализацией есть возможность использовать:
- централизованные логи.
- поиск/фильтры. 
- дать доступ разработчикам в UI.
- Использовать сохранённые запросы со ссылками.
- буферизация на диск для гарантии доставки.


# Задача 3: Мониторинг

### Концепция:
1 - На все сервера и узлы кластера ставим Grafana Agent.

2 - Агенты сами собирает метрики про машину(CPU, RAM, диск, сеть). Снимают метрики по каждому сервису (сколько CPU/RAM/диск). Забирают специфичные метрики сервисов, если они отдают их по стандартному адресу.

3 - Агент отправляет все метрики в Grafana.

4 - В Grafana строим панели и дашборды. Делаем запросы по метрикам.

Такой вариант прост в настройке, даёт минимальную нагрузку на команду, прост и надёжен в использовании. 